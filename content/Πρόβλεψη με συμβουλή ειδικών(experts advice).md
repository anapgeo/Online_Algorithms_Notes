
## Εισαγωγή

Έστω ότι θέλουμε να προβλέψουμε τον καιρό και απευθυνόμαστε σε κάποιους "ειδικούς"(experts) για συμβουλή. Οι προβλέψεις τους και η πρόβλεψη του αλγορίθμου ανα μέρα καθώς και τα πραγματικά αποτελέσματα δίνονται στον παρακάτω πίνακα

|         | BBC weather | Google Weather | γείτονας | πρόβλεψη αλγορίθμου | τι πραγματικά έγινε |
|---------|-------------|----------------|----------|---------------------|---------------------|
| ημέρα 1 | ν           | ο              | ν        | ν                   | ν                   |
| ημέρα 2 | ν           | ο              | ν        | ο                   | ν                   |
| ημέρα 3 | ο           | ν              | ο        | ν                   | ο                   |
| ημέρα 4 | ο           | ο              | ο        | ο                   | ο                   |

Το ερώτημα που τίθεται είναι: Μπορούμε να προβλέψουμε όσο καλά προέβλεψε ο καλύτερος εκ των υστέρων ειδικός;(Το Google Weather στη συγκεκριμένη περίπτωση)

> Σημείωση: Οι experts δεν έιναι απαραίτητα κάποιοι που έχουν γνώσεις αλλά οποιοσδήποτε έχει γνώμη(όπως ο γείτονας)

## Το μοντέλο

Έστω ότι υπάρχουν Ν experts, και εξετάζουμε T γύρους προβλέψεων. Σε κάθε γύρο t:

-  ο αλγόριθμος δέχεται ένα στιγμιότυπο δεδομένων $x_t \in \mathcal{X}$
-  λαμβάνει μια συμβουλή(advice) $\hat{y}_{it}\in \mathcal{Y}$ απο κάθε ένα από τους N experts
- κάνει μια πρόβλεψη $\hat{y}_t\in \mathcal{Y}$
- μετά την πρόβλεψη μαθαίνει την πραγματική ετικέτα $y_t \in \mathcal{Y}$
- προκύπτει κόστος $L(\hat{y}_t,y_t)$

Στόχος: Ελαχιστοποίηση Regret

-  Συγκρίνει το συνολικό κόστος του αλγορίθμου με το συνολικό κόστος του καλύτερου (εκ των υστέρων) expert
- Άρα το συνολικό κόστος θα είναι $\sum_{t=1}^{T}L(\hat{y}_t,y_t)-\min_i \sum_{t=1}^{T}L(\hat{y}_{it},y_t)$ (δηλαδή η διαφορά του συνολικού κόστους του αλγορίθμου με το συνολικό κόστος του καλύτερου εκ των υστέρων expert)

## Αλγόριθμος Halving: Αλάνθαστος expert


Ας θεωρήσουμε $\mathcal{Y}=\{0,1\}$ και $L(\hat{y},y)=|\hat{y}-y|$(δηλαδή classification πρόβλημα). Επίσης θεωρούμε πως υπάρχει κάποιος αλάνθαστος expert (δεν ξέρουμε ποιος).

Μοντέλο Φράγματος Σφαλμάτων (Mistake Bound model):

-  Πόσα λάθη χρειάζονται για να μάθουμε μια έννοια (τον τέλειο expert);
-  Μόλις ανακαλύψουμε τον σωστό expert (concept) δεν θα ξανακάνουμε λάθη στο μέλλον
-  ο μέγιστος αριθμός λαθών για μια συγκεκριμένη έννοια: $M_{ALG}(c)=\max_{x_1,\dots,x_T} |mistakes(ALG,c)|$
- Για μια κλάση εννοιών $C$ έχουμε $M_{ALG}(C)=\max_{c\in C} M_{ALG}(c)$
- Ψάχνουμε να βρούμε αλγορίθμους που ελαχιστοποιούν το $M_{ALG}(C)$


### Ο αλγόριθμος 

![[Screenshot_7 1.png]]


> Σημείωση: Πρακτικά για κάθε γύρο προβλέψεων λαμβάνουμε τα δεδομένα $x_t$. Στη συνέχεια κάνουμε μια πρόβλεψη με βάση το majority vote από τις συμβουλές $H_t$ των experts που "εμπιστευόμαστε". Αρχικά περιλαμβάνονται σε αυτούς όλοι οι διαθέσιμοι experts. Στη συνέχεια, μετά από κάθε πρόβλεψη, αν η πρόβλεψη που κάναμε ήταν λάθος τότε αφαιρούμε όσους experts έκανα λάθος πρόβλεψη(οι οποίοι λόγω του majority vote θα είναι τουλάχιστον οι μισοί)

Πόσα σφάλματα απαιτούνται για την εύρεση του αλάνθαστου expert;

> Θεώρημα: Έστω H ένα πεπερασμένο σύνολο υποθέσεων(experts). Τότε:
> 
> $$
 M_{halving}(H)\leq \log_2|H|
$$


Απόδειξη

Ο αλγόριθμος προβλέπει χρησιμοποιώντας πλειοψηφική ψήφο από τους εναπομείναντες experts (υποθέσεις). Άρα κάθε φορά που η πρόβλεψη είναι λανθασμένη, αφαιρούνται τουλάχιστον οι μισοί experts. Αρα μετά από $\log_2 |H|$ σφάλματα έχει μείνει μόνο μία υπόθεση/ expert ο οποίος θα είναι ο αλάνθαστος





## Weighted Majority: Δεν υπάρχει αλάνθαστος expert

